import gymnasium as gym
import numpy as np
import torch
from tqdm import tqdm

# Neural network for function approximation
class ActorCritic(torch.nn.Module):
    def __init__(self, inputDims, outputDims):
        super(ActorCritic, self).__init__()
        
        actorLayers = [
            nn.Linear(inputDims, 32),
            nn.ReLU(),
            nn.Linear(32, 32),
            nn.ReLU(),
        ]

        # Mean and standard deviation of actions are found, for 
        # sampling actions from a normal distribution
        self.actorMean = nn.Sequential(
            nn.Linear(32, outputDims)
        )

        self.actorStdDev = nn.Sequential(
            nn.Linear(32, outputDims)
        )

        criticLayers = [
            nn.Linear(inputDims, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.ReLU(),
        ]

        # Actor and critic have different outputs (distribution vs loss)
        self.actor = nn.Sequential(*actorLayers)
        self.critic = nn.Sequential(*criticLayers)

        self.actorOptimiser = torch.optim.Adam(self.actor.parameters())
        self.criticOptimiser = torch.optim.Adam(self.critic.parameters())


        # Avoids type issues
        self.double()


    # Performs forward pass to calculate loss
    def forward(self, state):
        # sharedFeatures contains the actual values at nodes
        sharedFeatures = self.actor(torch.tensor(state))

        means = self.actorMean(sharedFeatures)
        stdDevs = torch.log(
            1 + torch.exp(self.actorStdDev(sharedFeatures))
        )

        criticValue = self.critic(torch.tensor(state))

        return means, stdDevs, criticValue


    def get_losses(
        self,
        rewards: torch.Tensor,
        logProbs: torch.Tensor,
        value_preds: torch.Tensor,
        entropy: torch.Tensor,
        masks: torch.Tensor,
        gamma: float,
        lam: float,
        entropyCoefficient: float,
        device: torch.device,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Computes the loss of a minibatch (transitions collected in one sampling phase) for actor and critic
        using Generalized Advantage Estimation (GAE) to compute the advantages (https://arxiv.org/abs/1506.02438).

        Args:
            rewards: A tensor with the rewards for each time step in the episode, with shape [numEpisodes, n_envs].
            logProbs: A tensor with the log-probs of the actions taken at each time step in the episode, with shape [numEpisodes, n_envs].
            value_preds: A tensor with the state value predictions for each time step in the episode, with shape [numEpisodes, n_envs].
            masks: A tensor with the masks for each time step in the episode, with shape [numEpisodes, n_envs].
            gamma: The discount factor.
            lam: The GAE hyperparameter. (lam=1 corresponds to Monte-Carlo sampling with high variance and no bias,
                                          and lam=0 corresponds to normal TD-Learning that has a low variance but is biased
                                          because the estimates are generated by a Neural Net).
            device: The device to run the computations on (e.g. CPU or GPU).

        Returns:
            criticLoss: The critic loss for the minibatch.
            actorLoss: The actor loss for the minibatch.
        """
        T = len(rewards)
        advantages = torch.zeros(T, self.n_envs, device=device)

        # compute the advantages using GAE
        gae = 0.0
        for t in reversed(range(T - 1)):
            td_error = (
                rewards[t] + gamma * masks[t] * value_preds[t + 1] - value_preds[t]
            )
            gae = td_error + gamma * lam * masks[t] * gae
            advantages[t] = gae

        # calculate the loss of the minibatch for actor and critic
        criticLoss = advantages.pow(2).mean()

        # give a bonus for higher entropy to encourage exploration
        actorLoss = (
            -(advantages.detach() * logProbs).mean() - entropyCoefficient * entropy.mean()
        )
        return (criticLoss, actorLoss)
    

    def updateNetwork(self, actorLoss, criticLoss):
        self.actorOptimiser.zero_grad()
        actorLoss.backward()
        self.actorOptimiser.step()

        self.criticOptimiser.zero_grad()
        criticLoss.backward()
        self.criticOptimiser.step()


    

# Carries out the steps of the A2C algorithm
class A2C:
    def __init__(self, inputDims, outputDims):
        # Hyperparameters set arbitrarily
        self.learningRate = 1e-4
        self.gamma = 0.99
        self.episodes = 1e-6

        # Probabilities stores the probability of taking a given action, 
        # rewards stores the reward of that action
        self.probabilities = []
        self.rewards = []

        self.network = ActorCritic(inputDims, outputDims)


    # Samples an action from the distribution according to mean
    #  and standard deviation of the estimated best action
    def chooseAction(self, state):
        means, stdDevs = self.network(state)

        # Defines a distibution, samples from it, then finds the 
        # probability of taking that action, for calculating loss
        distribution = Normal(means[0] + self.episodes, stdDevs[0] + self.episodes)
        action = distribution.sample()
        probability = distribution.log_prob(action)

        action = action.numpy()

        self.probabilities.append(probability)

        return action




# create a wrapper environment to save episode returns and episode lengths
env = gym.make("InvertedPendulum-v4")
envs_wrapper = gym.wrappers.RecordEpisodeStatistics(env)

criticLosses = []
actorLosses = []
entropies = []

numEpisodes = 1000

# use tqdm to get a progress bar for training
for episode in range(numEpisodes):
    # we don't have to reset the envs, they just continue playing
    # until the episode is over and then reset automatically

    # reset lists that collect experiences of an episode (sample phase)
    episodeValues = torch.zeros(numEpisodes)
    episodeRewards = torch.zeros(numEpisodes)
    episodeLogProbs = torch.zeros(numEpisodes)
    masks = torch.zeros(numEpisodes)

    # at the start of training reset all envs to get an initial state
    if episode == 0:
        states, info = envs_wrapper.reset()

    # play n steps in our parallel environments to collect data
    for step in range(numEpisodes):
        # select an action A_{t} using S_{t} as input for the agent
        actions, logProbs, valuePredictions, entropy = agent.select_action(
            states
        )

        # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}
        states, rewards, terminated, truncated, infos = envs_wrapper.step(
            actions.cpu().numpy()
        )

        episodeValues[step] = torch.squeeze(valuePredictions)
        episodeRewards[step] = torch.tensor(rewards, device=device)
        episodeLogProbs[step] = logProbs

        # add a mask (for the return calculation later);
        # for each env the mask is 1 if the episode is ongoing and 0 if it is terminated (not by truncation!)
        masks[step] = torch.tensor([not term for term in terminated])

    # calculate the losses for actor and critic
    criticLoss, actorLoss = agent.get_losses(
        episodeRewards,
        episodeLogProbs,
        episodeValues,
        entropy,
        masks,
        gamma,
        lam,
        entropyCoefficient,
        device,
    )

    # update the actor and critic networks
    agent.update_parameters(criticLoss, actorLoss)

    # log the losses and entropy
    criticLosses.append(criticLoss.detach().cpu().numpy())
    actorLosses.append(actorLoss.detach().cpu().numpy())
    entropies.append(entropy.detach().mean().cpu().numpy())